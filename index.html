<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arman Zarei</title>
    <link rel="stylesheet" type="text/css" href="style.css">
</head>

<body>
    <div id="container">
        <div id="head-container">
            <div id="description-container">
                <h1>Arman Zarei</h1>
                <p>I am a second-year Computer Sceince Ph.D. student at the University of Maryland, advised by <a href="https://scholar.google.com/citations?user=lptAmrMAAAAJ&hl=en" target="_blank">Soheil Feizi</a>.</p>
                <p>My research focuses on Image Generative Models and Vision-Language Models, with a particular interest in understanding and interpreting how these models function. I am especially interested in enhancing compositional abilities, accelerating inference, localizing and editing knowledge, unlearning undesirable information, and improving image editing capabilities in text-to-image generative models.</p>
                <p>In the past, my research has spanned various domains, including Deep Learning Robustness, 3D Vision, and the application of Machine Learning to address neuroscientific challenges, such as improving the performance of seizure detection devices.</p>
                <p class="text-center">
                    <a href="mailto:azarei@umd.edu">Email</a>
                    /
                    <a href="https://scholar.google.com/citations?user=4FiZJokAAAAJ&hl=en" target="_blank">Google Scholar</a>
                    /
                    <a href="https://www.linkedin.com/in/arman-zarei-02b82994/" target="_blank">Linkedin</a>
                    /
                    <a href="https://github.com/ArmanZarei" target="_blank">Github</a>
                    /
                    <a href="files/CV_ArmanZarei.pdf" target="_blank">CV</a>
                </p>
            </div>
            <div id="image-container">
                <img src="images/arman_zarei.jfif" alt="">
            </div>
        </div>
        <section id="papers-container">
            <article class="paper-container">
                <div class="paper-thumbnail">
                    <img src="images/paper_images/Localizing_Knowledge_in_Diffusion_Transformers.jpg" alt="">
                </div>
                <div class="paper-details">
                    <a href="https://armanzarei.github.io/Localizing-Knowledge-in-DiTs/" target="_blank">
                        <h3 class="paper-title">Localizing Knowledge in Diffusion Transformers</h3>
                    </a>
                    <p class="paper-authors">
                        <strong>Arman Zarei</strong>, <a href="https://scholar.google.com/citations?user=6aRwDecAAAAJ&hl=en" target="_blank">Samyadeep Basu</a>, <a href="https://scholar.google.com/citations?user=NsJKrKIAAAAJ&hl=en" target="_blank">Keivan Rezai</a>, <a href="https://scholar.google.com/citations?user=4h_A4n4AAAAJ" target="_blank">Zihao Lin</a>, <a href="https://scholar.google.com/citations?user=K8w4dj4AAAAJ" target="_blank">Sayan Nag</a>, <a href="https://scholar.google.com/citations?hl=en&user=lptAmrMAAAAJ" target="_blank">Soheil Feizi</a>
                    </p>
                    <span class="conf-or-jour-name">
                        <em>Under Review</em>
                    </span>
                    <p class="paper-links">
                        <a href="https://armanzarei.github.io/Localizing-Knowledge-in-DiTs/" target="_blank">Project</a>,
                        <a href="https://arxiv.org/abs/2505.18832" target="_blank">Paper</a>
                    </p>
                    <p class="paper-abstract">
                        Our paper presents a method to localize where knowledge is encoded within Diffusion Transformers (DiT), enabling more interpretable and efficient model editing for personalization and knowledge unlearning.
                    </p>
                </div>
            </article>


            <article class="paper-container">
                <div class="paper-thumbnail">
                    <img src="images/paper_images/Understanding_and_Mitigating_Compositional_Issues_in_Text-to_Image_Generative_Models.png" alt="">
                </div>
                <div class="paper-details">
                    <a href="https://arxiv.org/abs/2406.07844v2" target="_blank">
                        <h3 class="paper-title">Improving Compositional Attribute Binding in Text-to-Image Generative Models via Enhanced Text Embeddings</h3>
                    </a>
                    <p class="paper-authors">
                        <strong>Arman Zarei*</strong>, <a href="https://scholar.google.com/citations?user=NsJKrKIAAAAJ&hl=en" target="_blank">Keivan Rezai*</a>, <a href="https://scholar.google.com/citations?user=6aRwDecAAAAJ&hl=en" target="_blank">Samyadeep Basu</a>, <a href="https://scholar.google.com/citations?hl=en&user=qCZacxgAAAAJ" target="_blank">Mehrdad Saberi</a>, <a href="https://scholar.google.com/citations?hl=en&user=4f4m6O0AAAAJ" target="_blank">Mazda Moayeri</a>, <a href="https://scholar.google.com/citations?hl=en&user=D9ebp-YAAAAJ" target="_blank">Priyatham Kattakinda</a>, <a href="https://scholar.google.com/citations?hl=en&user=lptAmrMAAAAJ" target="_blank">Soheil Feizi</a>
                    </p>
                    <span class="conf-or-jour-name">
                        <em>Under Review</em>
                    </span>
                    <p class="paper-links">
                        <a href="https://t2i-compositionality-wiclp.github.io/" target="_blank">Project</a>,
                        <a href="https://arxiv.org/abs/2406.07844v2" target="_blank">Paper</a>
                    </p>
                    <p class="paper-abstract">
                        Our paper demonstrates that text-to-image generative models often fail at accurately composing attributes and relationships due to sub-optimal text conditioning by the CLIP text-encoder, and we show that significant compositional improvements can be achieved by fine-tuning a simple linear projection on CLIP's representation space.
                    </p>
                </div>
            </article>

            <article class="paper-container">
                <div class="paper-thumbnail">
                    <img src="images/paper_images/enhancing-epileptic-seizure-detection-with-eeg-feature-embeddings.png" alt="">
                </div>
                <div class="paper-details">
                    <a href="#" target="_blank">
                        <h3 class="paper-title">Enhancing Epileptic Seizure Detection with EEG Feature Embeddings</h3>
                    </a>
                    <p class="paper-authors">
                        <strong>Arman Zarei</strong>, <a href="https://scholar.google.com/citations?user=RC9BVP8AAAAJ&hl=zh-CN" target="_blank">Bingzhao Zhu</a>, <a href="https://scholar.google.com/citations?user=9tu1zw4AAAAJ&hl=en" target="_blank">Mahsa Shoaran</a>
                    </p>
                    <span class="conf-or-jour-name">
                        <em>BioCAS</em>, 2023 <span class="red-text">(Oral Presentation)</span>
                    </span>
                    <p class="paper-links">
                        <a href="#" target="_blank">Paper</a>, <a href="#" target="_blank">Presentation</a>
                    </p>
                    <p class="paper-abstract">
                        Our approach enhances seizure detection in epilepsy patients by converting raw EEG data into informative embeddings, achieving state-of-the-art classification results with an event-based sensitivity of 100% and specificity of 99% on the CHB-MIT scalp EEG dataset.
                    </p>
                </div>
            </article>
            
            <article class="paper-container">
                <div class="paper-thumbnail justify-content-center">
                    <img src="images/paper_images/A_Data-Centric_Approach_for_Improving_Adversarial_Training_Through_the_Lens_of_Out-of-Distribution_Detection.png" alt="" class="width-90-percent">
                </div>
                <div class="paper-details">
                    <a href="https://ieeexplore.ieee.org/abstract/document/10105351" target="_blank">
                        <h3 class="paper-title">A Data-Centric Approach for Improving Adversarial Training Through the Lens of Out-of-Distribution Detection</h3>
                    </a>
                    <p class="paper-authors">
                        <a href="https://scholar.google.com/citations?user=DkjSgaUAAAAJ&hl=en" target="_blank">Mohammad Azizmalayeri*</a>, <strong>Arman Zarei*</strong>, Alireza Isavand, Mohammad Taghi Manzuri, <a href="https://scholar.google.com/citations?user=pRyJ6FkAAAAJ&hl=en" target="_blank">Mohammad Hossein Rohban</a>
                    </p>
                    <span class="conf-or-jour-name">
                        <em>CSICC</em>, 2023 <span class="red-text">(Oral Presentation)</span>
                    </span>
                    <p class="paper-links">
                        <a href="https://ieeexplore.ieee.org/abstract/document/10105351" target="_blank">Paper</a> / <a href="https://arxiv.org/abs/2301.10454" target="_blank">arXiv</a>
                    </p>
                    <p class="paper-abstract">
                        This research offers a data-centric approach to enhance machine learning model robustness against imperceptible adversarial perturbations by detecting and removing challenging samples during training, resulting in improved adversarial training.
                    </p>
                </div>
            </article>
            
            <article class="paper-container">
                <div class="paper-thumbnail justify-content-center">
                    <img src="images/paper_images/phyto_oracle.png" alt="" class="width-50-percent">
                </div>
                <div class="paper-details">
                    <a href="https://www.frontiersin.org/articles/10.3389/fpls.2023.1112973/full" target="_blank">
                        <h3 class="paper-title">PhytoOracle: Scalable, modular phenomics data processing pipelines</h3>
                    </a>
                    <p class="paper-authors">
                        Emmanuel M. Gonzalez, <a href="https://scholar.google.com/citations?user=TWxcizkAAAAJ&hl=en" target="_blank">Ariyan Zarei</a>, Nathanial Hendler, Travis Simmons, <strong>Arman Zarei</strong>, Jeffrey Demieville, Robert Strand, Bruno Rozzi, Sebastian Calleja, Holly Ellingson, Michele Cosi, Sean Davey, Dean O. Lavelle, Maria JoseÂ´ Truco, <a href="https://scholar.google.com/citations?user=nanIeAYAAAAJ&hl=en" target="_blank">Tyson L. Swetnam</a>, <a href="https://scholar.google.com/citations?hl=en&user=CzRd9pQAAAAJ" target="_blank">Nirav Merchant</a>, Richard W. Michelmore, <a href="https://scholar.google.com/citations?hl=en&user=hXICZlUAAAAJ" target="_blank">Eric Lyons</a>, <a href="https://scholar.google.com/citations?hl=en&user=XM9JvlgAAAAJ" target="_blank">Duke Pauli</a>
                    </p>
                    <span class="conf-or-jour-name">
                        <em>Frontiers in Plant Science</em>, 2023
                    </span>
                    <p class="paper-links">
                        <a href="https://www.frontiersin.org/articles/10.3389/fpls.2023.1112973/full" target="_blank">Paper</a>
                    </p>
                    <p class="paper-abstract">
                        PhytoOracle (PO) introduces modular, scalable pipelines for processing large volumes of phenomics data, improving efficiency, enabling data fusion, and supporting multi-system trait extraction, with broad applicability across species and various data sources, including drone data.</p>
                    </p>
                </div>
            </article>
            
            <article class="paper-container">
                <div class="paper-thumbnail justify-content-center">
                    <img src="images/paper_images/your_out-of-distribution_detection_method_is_not_robust.png" alt="" class="width-80-percent">
                </div>
                <div class="paper-details">
                    <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/1f6591cc41be737e9ba4cc487ac8082d-Abstract-Conference.html" target="_blank">
                        <h3 class="paper-title">Your Out-of-Distribution Detection Method is Not Robust!</h3>
                    </a>
                    <p class="paper-authors">
                        <a href="https://scholar.google.com/citations?user=DkjSgaUAAAAJ&hl=en" target="_blank">Mohammad Azizmalayeri*</a>, <a href="https://scholar.google.com/citations?hl=en&user=2H6Wl4MAAAAJ" target="_blank">Arshia Soltani Moakhar</a>, <strong>Arman Zarei</strong>, <a href="https://scholar.google.com/citations?hl=en&user=vV6CUUAAAAAJ" target="_blank">Reihaneh Zohrabi</a>, Mohammad Taghi Manzuri, <a href="https://scholar.google.com/citations?user=pRyJ6FkAAAAJ&hl=en" target="_blank">Mohammad Hossein Rohban</a>
                    </p>
                    <span class="conf-or-jour-name">
                        <em>NeurIPS</em>, 2022
                    </span>
                    <p class="paper-links">
                        <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/1f6591cc41be737e9ba4cc487ac8082d-Abstract-Conference.html" target="_blank">Paper</a> / <a href="https://arxiv.org/abs/2209.15246" target="_blank">arXiv</a>
                    </p>
                    <p class="paper-abstract">
                        Our study identifies vulnerabilities in current out-of-distribution (OOD) detection methods and presents the Adversarially Trained Discriminator (ATD) as a robust alternative with improved performance.
                    </p>
                </div>
            </article>
        </section>
    </div>
</body>
</html>